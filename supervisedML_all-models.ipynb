{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Supervised Machine Learning</u>\n",
    "\n",
    "Supervised learning is a type of machine learning where an algorithm is trained on labeled data. The algorithm learns to map input data to the correct output (labels) by generalizing patterns from the training dataset, then it can predict outputs for unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `→ How Supervised Learning is Achieved`\n",
    "Supervised learning involves the following steps:\n",
    "\n",
    "1. Data Collection: Gather labeled data, where each input is paired with the correct output.\n",
    "2. Data Preprocessing: Clean the data, remove outliers, handle missing values, and scale features.\n",
    "3. Model Training: Feed the labeled data to a machine learning algorithm that learns the mapping between inputs and outputs.\n",
    "4. Prediction: Once trained, the model predicts outputs for new, unseen data.\n",
    "5. Model Evaluation: Assess the performance of the model using metrics like accuracy, precision, recall, F1-score, or RMSE (Root Mean Square Error) for regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `→ Types of Supervised Learning:`\n",
    "\n",
    "Supervised learning can be divided into two main categories: classification and regression.\n",
    "\n",
    "1. Classification: The output variable is categorical (e.g., spam detection, customer segmentation). Algorithms used for classification include logistic regression, decision trees, support vector machines (SVM), k-nearest neighbors (KNN), neural networks, and ensemble methods like random forests or gradient boosting.\n",
    "\n",
    "2. Regression: The output variable is continuous (e.g., housing price prediction, stock market analysis). Algorithms used for regression include linear regression, decision trees, support vector machines (SVM), neural networks, and ensemble methods like random forests or gradient boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `→ Modeling Process:`\n",
    "\n",
    "1. Data Splitting: Split the dataset into a training set (to train the model) and a test set (to evaluate performance).\n",
    "2. Model Selection: Choose a suitable algorithm (e.g., linear regression, decision trees).\n",
    "3. Model Training: The algorithm adjusts its internal parameters (weights in the case of neural networks) to minimize the error between predicted and actual labels using optimization techniques like gradient descent.\n",
    "4. Model Evaluation: Evaluate the model using metrics like accuracy, precision, recall, F1-score, or RMSE (Root Mean Square Error) for regression tasks. Evaluation of a supervised model involves:\n",
    "\n",
    "    * Confusion Matrix: Measures true positives, false positives, true negatives, and false negatives (for classification tasks).\n",
    "    * Precision, Recall, and F1-Score: Used for imbalanced classification tasks.\n",
    "    * Accuracy: Percentage of correct predictions over total predictions.\n",
    "    * R-Squared and RMSE: Commonly used for regression models to measure the closeness of predicted values to actual values.\n",
    "\n",
    "5. Improving Model Performance\n",
    "Based on the evaluation results, the following methods can improve performance:\n",
    "\n",
    "    * Hyperparameter Tuning: Adjust parameters like learning rate, regularization strength, etc.\n",
    "    * Feature Engineering: Create new features or transform existing ones for better predictive power.\n",
    "    * Cross-Validation: Use techniques like K-fold cross-validation to avoid overfitting.\n",
    "    * Regularization: Apply L1 (Lasso) or L2 (Ridge) regularization to reduce model complexity.\n",
    "    * Ensemble Methods: Use techniques like Bagging (Random Forest) or Boosting (AdaBoost) to combine multiple weak models.\n",
    "    * Transfer Learning: Use pre-trained models (e.g., VGG16, ResNet50) and fine-tune their weights on a new dataset.\n",
    "    * Deep Learning: Train deep learning models like convolutional neural networks (CNNs) or recurrent neural networks (RNNs) to capture complex patterns and relationships in the data.\n",
    "    * Bayesian Optimization: Use Bayesian optimization techniques to find the best hyperparameters for a machine learning model.\n",
    "    * AutoML: Use automated machine learning tools like TPOT (Tree-based Pipeline Optimization Tool) or Auto-sklearn to automatically search for the best model and hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `→ Use Cases of Supervised Learning:`\n",
    "1. Banking and Finance: Predicting customer defaults using logistic regression or decision trees.\n",
    "2. Healthcare: Predicting medical conditions using logistic regression or neural networks.\n",
    "3. Telecommunications: Predicting customer churn using decision trees or random forests.\n",
    "4. Retail and E-commerce: Predicting customer purchases using logistic regression or neural networks.\n",
    "5. Social Media and Marketing: Analyzing user behavior using logistic regression or neural networks.\n",
    "6. Product Recommendations: Predicting products users might like using collaborative filtering or content-based filtering.\n",
    "7. Email Classification: Classifying emails as personal, work, or social using Naive Bayes or SVM.\n",
    "7. Social Media Monitoring: Monitoring user behavior using logistic regression or neural networks.\n",
    "8. Sentiment Analysis: Analyzing customer feedback using logistic regression or neural networks.\n",
    "9. Image Classification: Recognizing objects in images using convolutional neural networks (CNNs).\n",
    "10. Customer Segmentation: Grouping customers based on their behavior using clustering algorithms like K-means or hierarchical clustering.\n",
    "11. Anomaly Detection: Identifying unusual patterns or events in data using statistical methods or machine learning algorithms.\n",
    "12. Spam Detection: Classifying emails as spam or not spam using Naive Bayes or SVM.\n",
    "13. Fraud Detection: Identifying fraudulent transactions using decision trees or random forests.\n",
    "14. Medical Diagnosis: Predicting diseases based on patient symptoms using logistic regression or neural networks.\n",
    "15. Image Classification: Recognizing objects in images using convolutional neural networks (CNNs).\n",
    "16. Customer Churn Prediction: Identifying customers likely to leave using decision trees or random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `→ Techniques Involved in Supervised Learning:`\n",
    "\n",
    "1. Classification: Predict categorical labels (e.g., spam vs. not spam).\n",
    "2. Regression: Predict continuous values (e.g., house prices).\n",
    "3. Clustering: Group similar data points together (e.g., customer segmentation).\n",
    "4. Regularization: L1 (Lasso) and L2 (Ridge) regularization to avoid overfitting.\n",
    "5. Feature Engineering: Techniques like one-hot encoding, label encoding, or binning to transform categorical variables into numerical features.\n",
    "6. Dimensionality Reduction: Techniques like Principal Component Analysis (PCA) to reduce feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `→ List of Models in Supervised Learning:`\n",
    "\n",
    "1. Classification Models - In classification tasks, the goal is to predict discrete labels or categories. The model classifies input data into one of several predefined categories. Common algorithms include:\n",
    "    - Logistic Regression\n",
    "    - Decision Trees\n",
    "    - Random Forests\n",
    "    - Support Vector Machines(SVM)\n",
    "    - Naive Bayes\n",
    "    - K-Nearest Neighbors (KNN)\n",
    "    - Neural Networks (e.g., Feedforward Neural Networks, Recurrent Neural Networks)\n",
    "\n",
    "2. Regression Models - In regression tasks, the goal is to predict continuous values. The model learns from the labeled training data and predicts outputs that fall on a continuous spectrum. Common algorithms include:\n",
    "    - Linear Regression\n",
    "    - Polynomial Regression\n",
    "    - Ridge Regression\n",
    "    - Lasso Regression\n",
    "    - Support Vector Regression(SVR)\n",
    "    - Decision Trees(for regression)\n",
    "    - Bayesian Linear Regression\n",
    "\n",
    "3. Ensemble Methods - Ensemble methods in machine learning combine the predictions of multiple models (usually called \"base learners\") to improve the overall performance. The idea is that by aggregating the outputs of different models, the ensemble reduces errors and improves generalizability. Some common types of ensemble methods include:\n",
    "    - Bagging (Bootstrap Aggregating): It builds several independent models by training them on different random subsets of the data, and their predictions are averaged (or voted) to create a final output.\n",
    "    - Boosting: An ensemble technique where models are built sequentially, each new model focusing on the mistakes made by previous models. The goal is to turn weak learners into a strong learner by assigning more weight to incorrectly classified instances. Common boosting algorithms include:\n",
    "        - AdaBoost (Adaptive Boosting)\n",
    "        - Gradient Boosting\n",
    "        - XGBoost\n",
    "        - LightGBM\n",
    "        - CatBoost\n",
    "        - Deep Learning (e.g., Convolutional Neural Networks, Recurrent Neural Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `→ Real-World Examples:`\n",
    "1. Netflix: Uses supervised learning to recommend movies based on user preferences and past viewing history.\n",
    "2. Amazon: Predicts product recommendations based on customer data.\n",
    "3. Credit Scoring: Banks use supervised models to predict the likelihood of loan defaults."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `→ Conclusion:`\n",
    "\n",
    "Supervised learning is a powerful and widely used technique in machine learning, with various applications in various domains. By following the steps outlined above, you can build a solid understanding of supervised learning and leverage its benefits to achieve accurate predictions. Remember to apply feature engineering, cross-validation, and regularization techniques to improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `→ 🌐Sources:`\n",
    "1. geeksforgeeks.org - Supervised Machine Learning\n",
    "2. developer.ibm.com - Supervised learning models\n",
    "3. medium.com - List of Machine Learning Models\n",
    "4. scikit-learn.org - Supervised learning\n",
    "5. wikipedia.org - Supervised learning\n",
    "6. oreilly.com - Supervised Learning: Models and Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Starting with the Classification Models</u> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic regression is a supervised learning algorithm used for binary classification tasks, where the output is a discrete value (0 or 1, True or False). It is used when the dependent variable is categorical. Logistic regression models the probability that a given input belongs to a particular class using a logistic (sigmoid) function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `→ Modeling Process:`\n",
    "\n",
    "The modeling process includes:\n",
    "\n",
    "* Step 1: Define the problem as a binary classification task.\n",
    "\n",
    "* Step 2: Train the model by finding the best coefficients that fit the data. Logistic regression uses maximum likelihood estimation to find the optimal parameters.\n",
    "\n",
    "* Step 3: Predict probabilities using the logistic function:\n",
    "\n",
    "    <img src=\"./img/Screenshot1.png\"><br/>\n",
    "    where \"𝑤\" is the vector of coefficients, \"𝑥\" is the input, and \"b\" is the bias term.\n",
    "\n",
    "    `Note:` Here's a breakdown of each component\n",
    "    1. y^ - This is the predicted probability that the output belongs to a particular class (typically the positive class). It will always fall between 0 and 1 due to the properties of the sigmoid function.\n",
    "    2. σ(): The sigmoid function transforms the linear combination of inputs and coefficients into a probability. As 𝑧 approaches infinity, \n",
    "    𝜎(𝑧) approaches 1; as 𝑧 approaches negative infinity, 𝜎(𝑧) approaches 0.\n",
    "    3. w: This is the vector of coefficients (weights) that represents the strength and direction of the relationship between each input feature and the outcome.\n",
    "    4. 𝑥: This is the input feature vector. Each feature contributes to the prediction based on its corresponding weight in 𝑤.\n",
    "    5. b: The bias term allows the model to fit the data more flexibly by shifting the sigmoid curve left or right.\n",
    "    6. wT𝑥 + b: This represents a linear combination of the input features, adjusted by the weights and bias. The result is fed into the sigmoid function to yield a probability.\n",
    "\n",
    "        The logistic regression model uses this probability to classify inputs into one of the two classes, typically setting a threshold (often 0.5) to decide the final classification outcome.\n",
    "* Step 4: Decision boundary: Convert the probabilities into class labels by applying a threshold (typically 0.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `→ Maths Involved:`\n",
    "\n",
    "* <img src=\"./img/Sigmoid_func.png\"><br/>\n",
    "    <br/>\n",
    "* <img src=\"./img/Log-Loss_func.png\"><br/>\n",
    "    <br/>\n",
    "* <img src=\"./img/Gradient_descent.png\"><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `→ Use Cases of Logistic Regression:`\n",
    "\n",
    "1. Medical Diagnosis: Predicting whether a patient has a disease (e.g., cancer detection).\n",
    "2. Marketing: Customer churn prediction, determining whether a customer will buy a product.\n",
    "3. Finance: Credit card fraud detection or predicting loan defaults.\n",
    "4. Email Spam Detection: Classifying emails as spam or not spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `→ Techniques in Logistic Regression:`\n",
    "\n",
    "1. Binary Logistic Regression: The basic form, where the output is a binary class (0 or 1).\n",
    "2. Multinomial Logistic Regression: An extension to handle multi-class classification problems.\n",
    "3. Regularized Logistic Regression:\n",
    "    * L1 (Lasso): Adds an absolute value penalty to the weights.\n",
    "    * L2 (Ridge): Adds a squared penalty to the weights to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `→ Models Used in Logistic Regression:`\n",
    "\n",
    "1. Logistic Regression Model (Binary): Used for binary classification.\n",
    "2. Multinomial Logistic Regression: Used for multi-class classification.\n",
    "3. Regularized Logistic Regression (Lasso, Ridge): Used when there is a need to penalize large coefficients and reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `→ Real-World Examples:`\n",
    "\n",
    "1. Healthcare: Logistic regression is widely used to predict the likelihood of diseases based on patient data (e.g., heart disease, diabetes prediction).\n",
    "2. Finance: Used to predict credit card fraud or whether a customer will default on a loan.\n",
    "3. Social Media: Predicting user engagement or click-through rates for advertisements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `→ Implementation:` \n",
    "\n",
    "The implementation of Logistic Regression is in the folder ClassificationModels > logistic_regression.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
